{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "機械学習.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNe+aDOv/opQLhdlYQknJmW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junya-ph/report/blob/making_report/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppTC86tysQbJ"
      },
      "source": [
        "### 線形回帰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcJCZinA9I52"
      },
      "source": [
        "回帰問題：ある入力から出力を予測する問題\n",
        "\n",
        "・直線で予測→線形回帰\n",
        "\n",
        "・曲線で予測→非線形回帰\n",
        "\n",
        "回帰で扱うデータ\n",
        "\n",
        "・入力(説明変数、特徴量)→m次元のベクトル\n",
        "\n",
        "・出力(目的変数)→スカラー値\n",
        "\n",
        "線形回帰モデル\n",
        "\n",
        "・回帰問題を解くための機械学習モデルの一つ\n",
        "\n",
        "・教師あり学習\n",
        "\n",
        "・入力とm次元パラメータの線型結合を出力するモデル\n",
        "\n",
        "・線型結合→入力とパラメータの内積\n",
        "\n",
        "・モデルのパラメータ→特徴量が予測値に対して、どのように影響を与えるかを決定する重みの集合であり、切片はy軸との交点を表す\n",
        "\n",
        "・単回帰モデル→データは回帰直線に誤差が加わり観測されていると仮定\n",
        "\n",
        "・重回帰モデル→データは回帰曲線に誤差が加わり観測されていると仮定\n",
        "\n",
        "・データの分割→学習用データ、検証用データ\n",
        "\n",
        "・なぜ分割するのか→モデルへの汎化性能を測定するため\n",
        "\n",
        "・平均二乗誤差→パラメータのみに依存する関数であり、データは既知でパラメータのみ未知\n",
        "\n",
        "・最小二乗法→学習用データの平均二乗誤差を最小とするパラメータを探索\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmHeGkjBFNB3"
      },
      "source": [
        "以下は、線形回帰分析の実装結果となる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TvYRBKkqjQL"
      },
      "source": [
        "[![Image from Gyazo](https://i.gyazo.com/7fa6fa2ecb6fa6b2dd39a154c3b3aed2.png)](https://gyazo.com/7fa6fa2ecb6fa6b2dd39a154c3b3aed2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH5dTi7Izxxg"
      },
      "source": [
        "### 非線形回帰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bc-Rq4aCRLG"
      },
      "source": [
        "非線形回帰モデル\n",
        "\n",
        "・複雑な非線形構造を内在する現象に対して、非線形回帰モデリングを実施→データの構造を線形で捉えられる場合は限られるため、非線形な構造を捉えられる仕組みが必要\n",
        "\n",
        "・基底展開法→回帰関数として、基底関数と呼ばれる既知の非線形関数とパラメータベクトルの線型結合を使用し、未知パラメータは線形回帰モデルと同様に最小2乗法や最尤法により推定する\n",
        "\n",
        "・よく使われる基底関数→多項式関数、ガウス型基底関数、スプライン関数/ Bスプライン関数\n",
        "\n",
        "・未学習→学習データに対して、十分小さな誤差が得られないモデル\n",
        "\n",
        "【対策】モデルの表現力が低いため、表現力の高いモデルを利用する\n",
        "\n",
        "・過学習→小さな誤差は得られたが、テスト集合誤差との差が大きいモデル\n",
        "\n",
        "【対策】学習データの数を増やす、不要な基底関数(変数)を削除して表現力を抑止、正則化法を利用して表現力を抑止\n",
        "\n",
        "・不要な基底関数を削除→基底関数の数、位置やバンド幅によりモデルの複雑さが変化する\n",
        "\n",
        "・正則化(罰則化)→正則化項を課した関数を最小化する\n",
        "\n",
        "・正則化項→リッジ推定量(L2ノルムを利用)、ラッソ推定量(L1ノルムを利用)\n",
        "\n",
        "・汎化性能→学習に使用した入力だけでなく、これまで見たことのない新たな入力に対する予測性能\n",
        "\n",
        "・ホールドアウト法→有限のデータを学習用とテスト用の2つに分割し、「予測精度」や「誤り率」を推定する為に使用する\n",
        "\n",
        "・グリッドサーチ→全てのチューニングパラメータの組み合わせで評価値を算出し、最も良い評価値を持つチューニングパラメータを持つ組み合わせを採用する\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "266BIxVbFShW"
      },
      "source": [
        "以下は、非回帰分析の結果となる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jVSml57uLyY"
      },
      "source": [
        "[![Image from Gyazo](https://i.gyazo.com/d92265e1383fcf027040a9a3b7fdd057.png)](https://gyazo.com/d92265e1383fcf027040a9a3b7fdd057)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG0OtWGT1V1k"
      },
      "source": [
        "### ロジスティック回帰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh-0dY2-1ZK3"
      },
      "source": [
        "・分類問題(クラス分類)→ある入力(数値)からクラスに分類する問題\n",
        "\n",
        "・シグモイド関数→入力は実数、出力は必ず1~0、単調増加関数\n",
        "\n",
        "・同時確率→あるデータが得られた時、それが同時に得られる確率のこと\n",
        "\n",
        "・尤度関数→尤度関数を最大化するようなパラメータを選ぶ推定方法を最尤推定という\n",
        "\n",
        "・勾配降下法→反復学習によりパラメータを逐次的に更新する方法\n",
        "\n",
        "・確率的勾配降下法(SGD)→データを一つずつランダムに選んでパラメータを更新する\n",
        "\n",
        "・再現率→本当にpositiveなものからpositiveを予測できる割合\n",
        "\n",
        "・適合率→モデルがpositiveと予測したものの中で、本当にpositiveである割合\n",
        "\n",
        "・F値→再現率と適合率のどちらも高いモデルが良いモデルと言えるが、両者はトレードオフの関係であるため調和平均を取る"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9jHqnBiE-3b"
      },
      "source": [
        "以下のグラフは、ロジスティック回帰分析の結果となる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wb2KfM2zvFGC"
      },
      "source": [
        "[![Image from Gyazo](https://i.gyazo.com/c3201002e5bf99d36979a74f6b5cfdd8.png)](https://gyazo.com/c3201002e5bf99d36979a74f6b5cfdd8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOoqf9PB5fN0"
      },
      "source": [
        "### 主成分分析"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_u4WoYE5h78"
      },
      "source": [
        "・多変量データのもつ構造をより少数個の指標に圧縮\n",
        "\n",
        "・制約付き最適化問題の解き方→ラグランジュ関数を最大にする係数ベクトルを探索(微分して0になる点)し、元のデータの分散共分散行列の固有値と固有ベクトルが解となる\n",
        "\n",
        "・分散共分散行列は正定値対称行列→固有値は必ず0以上、固有ベクトルは直行\n",
        "\n",
        "・寄与率→第k主成分の分散の全分散に対する割合\n",
        "\n",
        "・累積寄与率→第1-k主成分まで圧縮した際の情報損失量の割合"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCNgBXpbEw0c"
      },
      "source": [
        "以下のグラフは、主成分分析の実装結果のとなる。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kM_HCzpExmz"
      },
      "source": [
        "[![Image from Gyazo](https://i.gyazo.com/b9a72c9922a594c8c26fc70e081167c0.png)](https://gyazo.com/b9a72c9922a594c8c26fc70e081167c0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdLWSj5j8713"
      },
      "source": [
        "### アルゴリズム"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6eCO_-a9FWq"
      },
      "source": [
        "**k近傍法**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e8WVR5g9vlS"
      },
      "source": [
        "・分類問題のための機械学習手法\n",
        "\n",
        "・最近傍のデータをk個取り、それらが最も多く所属するクラスに識別する\n",
        "\n",
        "・kを変化させると結果も変わり、kを大きくすると決定境界は滑らかになる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E1lNYgxERuR"
      },
      "source": [
        "以下のグラフは、k近傍法の実装結果になる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1zoWEDpEPDW"
      },
      "source": [
        "[![Image from Gyazo](https://i.gyazo.com/19580159dc0ba9c916469a57af8775d1.png)](https://gyazo.com/19580159dc0ba9c916469a57af8775d1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NSiVArY-QrW"
      },
      "source": [
        "k-**means**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeJkOS7Y-TFp"
      },
      "source": [
        "・教師なし学習、クラスタリング手法\n",
        "\n",
        "・クラスタリング→特徴の似ているもの同士をグループ化する\n",
        "\n",
        "k-meansのアルゴリズム\n",
        "\n",
        "1)各クラスタ中心の初期値を設定する\n",
        "\n",
        "2)各データの点に対して、各クラスタ中心との距離を計算し、最も距離が近いクラスタを割り当てる\n",
        "\n",
        "3)各クラスタの平均ベクトル(中心)を計算する\n",
        "\n",
        "4)収束するまで2)と3)の処理を繰り返す"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWCSBvIdEaYT"
      },
      "source": [
        "以下の表は、k-meansの実装結果になる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8bP88KjC9TU"
      },
      "source": [
        "[![Image from Gyazo](https://i.gyazo.com/1ccef7e20ab0e9d30dd240da1338a806.png)](https://gyazo.com/1ccef7e20ab0e9d30dd240da1338a806)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7Wqwkwa_L11"
      },
      "source": [
        "### サポートベクターマシン"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSmsfDo09lqf"
      },
      "source": [
        "・異なるクラスの各データ点との距離が最大となるような境界線を求めることでパターン分類を行う→距離を最大化することをマージン最大化という\n",
        "\n",
        "・SVMの問題→扱うデータは高次元、データが線形分類できない\n",
        "\n",
        "・２クラス分類問題→決定関数を用いる、得られた境界を分類境界という\n",
        "\n",
        "・ハードマージン法→余裕を持って線形分離が可能なものに利用できる\n",
        "\n",
        "・ソフトマージン法→データがきれいに分離してなくても利用できる\n",
        "\n",
        "・データの次元が大きくなっても識別精度が良く、最適化すべきパラメータが少ない。\n",
        "\n",
        "・学習データが多いと計算量が膨大になり、スケーリングが必要となる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQfaU45pCvwb"
      },
      "source": [
        "ハードマージン法で分離"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF49zVKtBGHD"
      },
      "source": [
        "[![Image from Gyazo](https://i.gyazo.com/a8c5f9ed24ec2385fec2e24d0a417c89.png)](https://gyazo.com/a8c5f9ed24ec2385fec2e24d0a417c89)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF6nu3HdCsKu"
      },
      "source": [
        "ソフトマージン法で分離"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZ0Zvu8MChQS"
      },
      "source": [
        "[![Image from Gyazo](https://i.gyazo.com/90d5db1366d40e0d1569e9fd0c3f3b08.png)](https://gyazo.com/90d5db1366d40e0d1569e9fd0c3f3b08)"
      ]
    }
  ]
}